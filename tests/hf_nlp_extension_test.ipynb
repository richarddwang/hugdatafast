{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from fastai.callback.core import *\n",
    "\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import ElectraTokenizerFast\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-generator\")\n",
    "from hugdatafast import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple tokenization & infer cache name\n",
    "`cols`(`Dict[str]`): tokenize the every column named key into column named its value  \n",
    "`cols`(`List[str]`): specify the name of columns to be tokenized, replace the original columns' data with tokenized one\n",
    "\n",
    "Here, we tokenized \"sentence\" into a new column named \"text_idxs\", the \"sentence\" column still exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Reusing dataset glue (/home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#1', max=4275.0, style=ProgressStyle(description_width='i…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "664fbd6cc00b44848cf79d99445e02d1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#0', max=4276.0, style=ProgressStyle(description_width='i…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8bda3d5f98e4c7db9a2a9f024b2ab0f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#0', max=522.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd3364dce60a473ea3191cafd6835085"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#1', max=521.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1dcdbce0511b4c2a80bdf46561c3edc8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#0', max=532.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d738bb0ebfb441f4ad8cf7a7418ff38a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#1', max=531.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c3213746dbd42b9bf7995b3d12a84f3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n{'idx': 0, 'label': 1, 'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\", 'text_idxs': [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]}\n\n/home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tokenized_train_00000_of_00002.arrow\n/home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tokenized_validation_00000_of_00002.arrow\n/home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tokenized_test_00000_of_00002.arrow\n"
    }
   ],
   "source": [
    "\n",
    "cola = datasets.load_dataset('glue', 'cola')\n",
    "tokenized_cola = cola.my_map(SimpleTokenize({'sentence':'text_idxs'}, hf_tokenizer),\n",
    "                             cache_file_names='tokenized_{split}', num_proc=2)\n",
    "print(tokenized_cola['train'][0])\n",
    "print()\n",
    "for dset in tokenized_cola.values(): print(dset.cache_files[0]['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fastai `Dataloaders` and `show_batch`\n",
    "\n",
    "`cols`: **specify columns whose values form a output sample in order**, and the semantic type of each column to encode/decode, with one of the following signature (see doc).\n",
    "\n",
    "Here, `['text_idxs, 'label']` is equal to `{'text_idxs': TensorText, 'label': TensorCategory}`\n",
    "\n",
    "The bars are sorting samples according to length, see `MySortedDL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Set __getitem__(key) output type to torch for ['text_idxs', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text_idxs', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n 98%|█████████▊| 1037/1063 [00:02<00:00, 515.45it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ron wanted to wear a tuxedo to the party, but wear a tuxedo to the party caspar couldn't decide whether to.</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer, neat_show=True)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32)\n",
    "cola_dls.show_batch(max_n=2) # show at most two rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either specify `neat_show=False` (which is default), to show real data which is tokenized and  with pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Set __getitem__(key) output type to torch for ['text_idxs', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text_idxs', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n 98%|█████████▊| 1040/1063 [00:02<00:00, 516.29it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever , worked in any office which contained any type ##writer which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>will put a picture of bill on your desk before tomorrow , this girl in the red coat will put a picture of bill on your desk before tomorrow . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols={'text_idxs': TensorText, 'label': TensorCategory}, hf_toker=hf_tokenizer)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32)\n",
    "cola_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_with_label` is `False` by default, so in test set the sample formed by only first `n_inp` columns specified, which is x.\n",
    "\n",
    "This make you able to apply the same to all splits when test set come with no y or fake y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple columns (> 2) in sample\n",
    "Some points to notice:\n",
    "- title of each column showed is and in order of `cols` specified in `HF_Datasets`\n",
    "- auto pad sequence to the max length in the batch, for all columns\n",
    "- If a fastai semantic tensor type is not specified, it look dtype and shape of the tensor and decide how to decode it autmatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/super_glue.py not found in cache or force_download set to True, downloading to /home/yisiang/.cache/huggingface/datasets/tmp4vauussu\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=10474.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16f232473c554b5da97721aa9feef900"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nstoring https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/super_glue.py in cache at /home/yisiang/.cache/huggingface/datasets/17727f4c5312e09bd16ee8581466c4f74b1802efd416965b4cfd523c12fad94d.ab72d3ffcbe0d0e93a4595f2a810b3988c20d7836ae0bdb5ff4bdccf6bd92a36.py\ncreating metadata file for /home/yisiang/.cache/huggingface/datasets/17727f4c5312e09bd16ee8581466c4f74b1802efd416965b4cfd523c12fad94d.ab72d3ffcbe0d0e93a4595f2a810b3988c20d7836ae0bdb5ff4bdccf6bd92a36.py\nhttps://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/dataset_infos.json not found in cache or force_download set to True, downloading to /home/yisiang/.cache/huggingface/datasets/tmpfw9s15jy\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9253.0, style=ProgressStyle(description…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "108f0522a50f4a5f9fbeb97db165fd46"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "storing https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/dataset_infos.json in cache at /home/yisiang/.cache/huggingface/datasets/be2c3836d8078b3465c52eebc3e437eeb18adabce99af20c98422a53acc7d3d4.9fa45241690c27df567c8014a4bf461a4ba1e82bd4358961888c6bf59769c3b5\ncreating metadata file for /home/yisiang/.cache/huggingface/datasets/be2c3836d8078b3465c52eebc3e437eeb18adabce99af20c98422a53acc7d3d4.9fa45241690c27df567c8014a4bf461a4ba1e82bd4358961888c6bf59769c3b5\nChecking /home/yisiang/.cache/huggingface/datasets/17727f4c5312e09bd16ee8581466c4f74b1802efd416965b4cfd523c12fad94d.ab72d3ffcbe0d0e93a4595f2a810b3988c20d7836ae0bdb5ff4bdccf6bd92a36.py for additional imports.\nFound main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/super_glue.py at /home/yisiang/.cache/huggingface/modules/datasets_modules/datasets/super_glue\nFound specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/super_glue.py at /home/yisiang/.cache/huggingface/modules/datasets_modules/datasets/super_glue/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de\nFound script file from https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/super_glue.py to /home/yisiang/.cache/huggingface/modules/datasets_modules/datasets/super_glue/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de/super_glue.py\nFound dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/dataset_infos.json to /home/yisiang/.cache/huggingface/modules/datasets_modules/datasets/super_glue/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de/dataset_infos.json\nFound metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/master/datasets/super_glue/super_glue.py at /home/yisiang/.cache/huggingface/modules/datasets_modules/datasets/super_glue/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de/super_glue.json\n\nLoading Dataset Infos from /home/yisiang/.cache/huggingface/modules/datasets_modules/datasets/super_glue/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de\nOverwrite dataset info from restored data version.\nLoading Dataset info from /home/yisiang/.cache/huggingface/datasets/super_glue/wsc.fixed/1.0.2/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de\nReusing dataset super_glue (/home/yisiang/.cache/huggingface/datasets/super_glue/wsc.fixed/1.0.2/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de)\nConstructing Dataset for split train, validation, test, from /home/yisiang/.cache/huggingface/datasets/super_glue/wsc.fixed/1.0.2/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de\n100%|██████████| 3/3 [00:00<00:00,  7.93it/s]\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nLoading cached processed dataset at /home/yisiang/.cache/huggingface/datasets/super_glue/wsc.fixed/1.0.2/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de/cache-4502eb5c5e2717ab.arrow\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nLoading cached processed dataset at /home/yisiang/.cache/huggingface/datasets/super_glue/wsc.fixed/1.0.2/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de/cache-813b86f371807b5c.arrow\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nLoading cached processed dataset at /home/yisiang/.cache/huggingface/datasets/super_glue/wsc.fixed/1.0.2/41d9edb3935257e1da4b7ce54cd90df0e8bb255a15e46cfe5cbc7e1c04f177de/cache-d3606f75f39a34a0.arrow\nSet __getitem__(key) output type to torch for ['text', 'span1_index', 'span1_text', 'span2_index', 'span2_text', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text', 'span1_index', 'span1_text', 'span2_index', 'span2_text', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n{'idx': 0, 'label': 0, 'span1_index': 0, 'span1_text': 'Mark', 'span2_index': 13, 'span2_text': 'He', 'text': 'Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical.'}\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>span1_index</th>\n      <th>span1_text</th>\n      <th>span2_index</th>\n      <th>span2_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mark told pete many lies about himself , which pete included in his book . he should have been more skeptical . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>0</td>\n      <td>mark [PAD] [PAD]</td>\n      <td>13</td>\n      <td>he</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mothers of arthur and celeste have come to the town to fetch them . they are very happy to have them back , but they sc ##old them just the same because they ran away . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>1</td>\n      <td>mothers [PAD] [PAD]</td>\n      <td>25</td>\n      <td>them</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mark was close to mr . singer ' s heels . he heard him calling for the captain , promising him , in the jar ##gon everyone talked that night , that not one thing should be damaged on the ship except only the ammunition , but the captain and all his crew had best stay in the cabin until the work was over</td>\n      <td>4</td>\n      <td>mr . singer</td>\n      <td>8</td>\n      <td>he</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "wsc = datasets.load_dataset('super_glue', 'wsc.fixed')\n",
    "print(wsc['train'][0])\n",
    "tokenized_wsc = wsc.my_map(simple_tokenize_func(['text', 'span1_text', 'span2_text'], hf_tokenizer))\n",
    "wsc_dsets = HF_Datasets(tokenized_wsc, cols={'text': TensorText, 'span1_index': noop, 'span1_text':TensorText, 'span2_index': noop, 'span2_text': TensorText, 'label': lambda t: t.bool()}, # convert label (int) to (bool), just to test its abililty to show tensor(bool)\n",
    "hf_toker=hf_tokenizer)\n",
    "dls = wsc_dsets.dataloaders(bs=3, srtkey_fc=False, shuffle_train=False) # don't sort samples, don't shuffle trainset\n",
    "#bk()\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aggregate Dataset\n",
    "a sample in transformed dataset is aggregated/accumulated from multiple original samples.\n",
    "\n",
    "- Except for `LMTransform`, you can implement your own logic create a class inherits `AggregateTransform` and implements `accumulate` and `create_example` method\n",
    "\n",
    "- Note that you should pass **tokenized** dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make  dataset(s) for (traditional) language model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Set __getitem__(key) output type to python objects for ['text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nCaching processed dataset at /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-e6acb88170f61c6d.arrow\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2a6dfaea1344f0281695366de90d62a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nDone writing 481 examples in 157576 bytes /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tmppum5q2sm.\nSet __getitem__(key) output type to python objects for [] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to python objects for ['x_text', 'y_text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nOriginal dataset:\nnum of samples: 1043\nsecond to last sentence: John arranged for himself to get the prize.\n          last sentence: John talked to Bill about himself.\nLM dataset:\nnum of sampels: 481\nlast text (x): . john talked to bill about himself\nlast text (y): john talked to bill about himself.\n"
    }
   ],
   "source": [
    "cola_val = tokenized_cola['validation']\n",
    "#bk()\n",
    "lm_cola_val = LMTransform(cola_val, max_len=20, text_col='text_idxs').map()\n",
    "\n",
    "print('Original dataset:')\n",
    "print('num of samples:', len(cola['validation']))\n",
    "print('second to last sentence:', cola['validation'][-2]['sentence'])\n",
    "print('          last sentence:', cola['validation'][-1]['sentence'])\n",
    "print('LM dataset:')\n",
    "print('num of sampels:', len(lm_cola_val))\n",
    "assert len(lm_cola_val) == 481\n",
    "print('last text (x):', hf_tokenizer.decode(lm_cola_val[-1]['x_text']))\n",
    "print('last text (y):', hf_tokenizer.decode(lm_cola_val[-1]['y_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Set __getitem__(key) output type to python objects for ['text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to python objects for ['text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to python objects for ['text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nCaching processed dataset at /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-2509c56d4d553502.arrow\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c634eed05df4c81b7185ed05fc4b585"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nDone writing 1564 examples in 1263672 bytes /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tmp6hips0lj.\nSet __getitem__(key) output type to python objects for [] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to python objects for ['x_text', 'y_text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nCaching processed dataset at /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-b25d35199b6c7232.arrow\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83998357277546c59d3bc3204272927f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Done writing 198 examples in 159840 bytes /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tmplm69lznx.\nSet __getitem__(key) output type to python objects for [] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to python objects for ['x_text', 'y_text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nCaching processed dataset at /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-888ff239e1f71e93.arrow\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5adc15037124d3e89a390bd83d812ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Done writing 200 examples in 161056 bytes /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tmpigpvusur.\nSet __getitem__(key) output type to python objects for [] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to python objects for ['x_text', 'y_text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['x_text', 'y_text'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_text</th>\n      <th>y_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey . the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as you eat the most , you want the</td>\n      <td>sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey . the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as you eat the most , you want the least</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>. the more you would want , the less you would eat . i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead , she gets depressed . the ang ##rier mary got , the more she looked at pictures</td>\n      <td>the more you would want , the less you would eat . i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead , she gets depressed . the ang ##rier mary got , the more she looked at pictures .</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "lm_cola = LMTransform(tokenized_cola, max_len=50, text_col='text_idxs').map()\n",
    "# test single dataset\n",
    "lm_ds = HF_Dataset(lm_cola['validation'], cols={'x_text':LMTensorText, 'y_text':TensorText},hf_toker=hf_tokenizer)\n",
    "lm_dl = MySortedDL(lm_ds, srtkey_fc=False)\n",
    "lm_dl.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ELECTRA data creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Set __getitem__(key) output type to python objects for ['sentence'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nTesting the mapped function outputs\nTesting finished, running the mapping function on the dataset\nCaching processed dataset at /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-7db22ab214e91040.arrow\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbec5b14896f4efc93a66ef705397304"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Done writing 78 examples in 78488 bytes /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/tmpi_20vusq.\n\nSet __getitem__(key) output type to python objects for [] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to python objects for ['input_ids', 'sentA_lenth'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['input_ids', 'sentA_lenth'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>sentA_lenth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] no writer , nor any playwright , meets in vienna . that you will marry any student is not certain . felicia kicked the ball off the bench . i sent the package halfway around the world . sam gave the ball out of the basket . sam offered the ball out of the basket . park square has a fest ##ive air . [SEP] the worker will have a job . no one can forgive that comment to you . we launched the rocket to the moon , but it blew up before it got there . sarah promised catherine her old car , but then gave it to her son instead . i lent the book part ##way to tony . the farmer loaded [SEP]</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS] i borrowed fred ' s diagram of a snake ' s eye because steve ' s had been stolen . jerry attempted to blow up the pentagon . so fast did he run that nobody could catch him . bill bought a red house , and max bought one too . who always drinks milk ? the book which inspired them was very long . [SEP] the book what inspired them was very long . i know the person whose mother died . the person whose mother ' s dog we were all fond of . i wonder whose mother died . i wonder whose mother ' s dog died . i wonder to whom they dedicated the building . give me the phone number of [SEP]</td>\n      <td>67</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "proc_dset = ELECTRADataTransform(cola['validation'], is_docs=False, text_col='sentence', max_length=128, hf_toker=hf_tokenizer).map()\n",
    "e_dsets = HF_Datasets({'train':proc_dset}, cols={'input_ids':TensorText,'sentA_lenth':noop}, hf_toker=hf_tokenizer)\n",
    "e_dls = e_dsets.dataloaders(srtkey_fc=False)\n",
    "e_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test filtering feature\n",
    "Note that filter won't be applied to split other than train, because validation/test set is for fair comparison, and you can't take out samples at your will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'train': 26, 'validation': 2, 'test': 6}\n"
    }
   ],
   "source": [
    "l = 23\n",
    "num = {}\n",
    "for split in tokenized_cola:\n",
    "  num[split] = reduce(lambda sum, sample: sum+(1 if len(sample['text_idxs'])==l else 0), \n",
    "                      tokenized_cola[split], 0)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Set __getitem__(key) output type to torch for ['text_idxs', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text_idxs', 'label'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to torch for ['text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\n 99%|█████████▉| 1054/1063 [00:02<00:00, 433.55it/s]Test passed\n"
    }
   ],
   "source": [
    "ccola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer)\n",
    "ccola_dls = ccola_dsets.dataloaders(filter_fc=lambda text_idxs, label: len(text_idxs)!=l,)\n",
    "\n",
    "for i, split in enumerate(tokenized_cola):\n",
    "  if split == 'train':\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split])-num[split],f\"{split}: filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}, should be filtered: {num[split]}\"\n",
    "  else:\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split]), f\"{split}: accidentally filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cache dataloader\n",
    "If sorting or filtering is applied, dataloader need to create some record inside it, to do it only once, we can cache the records. \n",
    "\n",
    "If `cache_dir` is not specified, it will be the cache_dir of `dsets` passed to `HF_Datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "99%|█████████▉| 1054/1063 [00:02<00:00, 420.38it/s]"
    }
   ],
   "source": [
    "for f in ['/tmp/cached_train.json','/tmp/cached_val.json', '/tmp/cached_test.json']:\n",
    "  if Path(f).exists(): os.remove(f)\n",
    "\n",
    "ccola_dls = ccola_dsets.dataloaders(cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we load the caches, it should be fast and progress bars sholdn't appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccola_dls = ccola_dsets.dataloaders(cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}