{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace as bk\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "import nlp\n",
    "from transformers import ElectraTokenizerFast\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-generator\")\n",
    "from hugdatafast import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize all splits of dataset at once\n",
    "`cols`(`Dict[str]`): tokenize the every column named key into column named its value  \n",
    "`cols`(`List[str]`): specify the name of columns to be tokenized, replace the original columns' data with tokenized one\n",
    "\n",
    "Here, we tokenized \"sentence\" into a new column named \"text_idxs\", the \"sentence\" column still exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'idx': 0,\n 'label': 1,\n 'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n 'text_idxs': [2256,\n  2814,\n  2180,\n  1005,\n  1056,\n  4965,\n  2023,\n  4106,\n  1010,\n  2292,\n  2894,\n  1996,\n  2279,\n  2028,\n  2057,\n  16599,\n  1012]}"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "cola = nlp.load_dataset('glue', 'cola') \n",
    "# cola is {'train':nlp.Dataset, 'validation':nlp.Dataset, 'test':nlp.Dataset}\n",
    "tokenized_cola = HF_TokenizeTfm(cola, {'sentence':'text_idxs'}, hf_tokenizer).map()\n",
    "tokenized_cola['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom function apply to all splits of dataset at once\n",
    "The `func` of `HF_Transform` is `function` in `nlp.Dataset.map`, but it will be applied to all splits individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'idx': 0,\n 'label': 1,\n 'sentence1': 'Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.',\n 'sentence2': 'Christopher Reeve had an accident.',\n 'tok_ids': [101,\n  11271,\n  20726,\n  1010,\n  1996,\n  7794,\n  1997,\n  1996,\n  3364,\n  5696,\n  20726,\n  1010,\n  2038,\n  2351,\n  1997,\n  11192,\n  4456,\n  2012,\n  2287,\n  4008,\n  1010,\n  2429,\n  2000,\n  1996,\n  5696,\n  20726,\n  3192,\n  1012,\n  102,\n  5696,\n  20726,\n  2018,\n  2019,\n  4926,\n  1012,\n  102]}"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "rte = nlp.load_dataset('glue', 'rte')\n",
    "# ax is {'test': nlp.Dataset}\n",
    "def custom_tokenize(example):\n",
    "  example['tok_ids'] = hf_tokenizer.encode(example['sentence1'], example['sentence2'])\n",
    "  return example\n",
    "tokenized_rte = HF_Transform(rte, custom_tokenize).map()\n",
    "tokenized_rte['validation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fastai `Dataloaders` and `show_batch`\n",
    "\n",
    "`cols`: **specify columns whose values form a output sample in order**, and the semantic type of each column to encode/decode, with one of the following signature (see doc).\n",
    "\n",
    "Here, `['text_idxs, 'label']` is equal to `{'text_idxs': TensorText, 'label': TensorCategory}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "98%|█████████▊| 1040/1063 [00:02<00:00, 384.49it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hank plays the guitar and finds arrangements for all the old folk songs which are still sung in these hills, and ernie writes down all the old folk songs which are still sung in these hills.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer, neat_show=True)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32)\n",
    "cola_dls.show_batch(max_n=2) # show at most two rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either specify `neat_show=False` (which is default), to show real data which is tokenized and  with pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "99%|█████████▉| 1053/1063 [00:03<00:00, 354.55it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever , worked in any office which contained any type ##writer which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sam picked those packages up which are to be mail ##ed tomorrow rest might , but he didn ' t want to do so until it had stopped raining . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols={'text_idxs': TensorText, 'label': TensorCategory}, hf_toker=hf_tokenizer)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32)\n",
    "cola_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_with_label` is `False` by default, so in test set the sample formed by only first `n_inp` columns specified, which is x.\n",
    "\n",
    "This make you able to apply the same to all splits when test set come with no y or fake y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple columns (> 2) in sample\n",
    "Some points to notice:\n",
    "- title of each column showed is and in order of `cols` specified in `HF_Datasets`\n",
    "- auto pad sequence to the max length in the batch, for all columns\n",
    "- If a fastai semantic tensor type is not specified, it look dtype and shape of the tensor and decide how to decode it autmatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'idx': 0, 'label': 0, 'span1_index': 0, 'span1_text': 'Mark', 'span2_index': 13, 'span2_text': 'He', 'text': 'Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical.'}\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>span1_index</th>\n      <th>span1_text</th>\n      <th>span2_index</th>\n      <th>span2_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mark told pete many lies about himself , which pete included in his book . he should have been more skeptical . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>0</td>\n      <td>mark [PAD] [PAD]</td>\n      <td>13</td>\n      <td>he</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mothers of arthur and celeste have come to the town to fetch them . they are very happy to have them back , but they sc ##old them just the same because they ran away . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>1</td>\n      <td>mothers [PAD] [PAD]</td>\n      <td>25</td>\n      <td>them</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mark was close to mr . singer ' s heels . he heard him calling for the captain , promising him , in the jar ##gon everyone talked that night , that not one thing should be damaged on the ship except only the ammunition , but the captain and all his crew had best stay in the cabin until the work was over</td>\n      <td>4</td>\n      <td>mr . singer</td>\n      <td>8</td>\n      <td>he</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "wsc = nlp.load_dataset('super_glue', 'wsc.fixed')\n",
    "print(wsc['train'][0])\n",
    "tokenized_wsc = HF_TokenizeTfm(wsc, ['text', 'span1_text', 'span2_text'], hf_tokenizer).map()\n",
    "wsc_dsets = HF_Datasets(tokenized_wsc, cols={'text': TensorText, 'span1_index': noop, 'span1_text':TensorText, 'span2_index': noop, 'span2_text': TensorText, 'label': lambda t: t.bool()}, # convert label (int) to (bool), just to test its abililty to show tensor(bool)\n",
    "hf_toker=hf_tokenizer)\n",
    "dls = wsc_dsets.dataloaders(bs=3, srtkey_fc=False, shuffle_train=False) # don't sort samples, don't shuffle trainset\n",
    "#bk()\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aggregate Dataset\n",
    "a sample in transformed dataset is aggregated/accumulated from multiple original samples.\n",
    "\n",
    "- Except for `LMTransform`, you can implement your own logic create a class inherits `AggregateTransform` and implements `accumulate` and `create_example` method\n",
    "\n",
    "- Note that you should pass **tokenized** dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make  dataset for (traditional) language model\n",
    "You can always pass dict of `nlp.Dataset` or a `nlp.Dataset` at your will for any transform class, we've test passing a dict, now we test a `nlp.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original dataset:\nnum of samples: 1043\nsecond to last sentence: John arranged for himself to get the prize.\n          last sentence: John talked to Bill about himself.\nLM dataset:\nnum of sampels: 481\nlast text (x): . john talked to bill about himself\nlast text (y): john talked to bill about himself.\n"
    }
   ],
   "source": [
    "cola_val = tokenized_cola['validation']\n",
    "lm_dataset = LMTransform(cola_val, max_len=20, text_col='text_idxs').map()\n",
    "\n",
    "print('Original dataset:')\n",
    "print('num of samples:', len(cola['validation']))\n",
    "print('second to last sentence:', cola['validation'][-2]['sentence'])\n",
    "print('          last sentence:', cola['validation'][-1]['sentence'])\n",
    "print('LM dataset:')\n",
    "print('num of sampels:', len(lm_dataset))\n",
    "assert len(lm_dataset) == 481\n",
    "print('last text (x):', hf_tokenizer.decode(lm_dataset[-1]['x_text']))\n",
    "print('last text (y):', hf_tokenizer.decode(lm_dataset[-1]['y_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_text</th>\n      <th>y_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey</td>\n      <td>sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey .</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less .</td>\n      <td>mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "lm_ds = HF_Dataset(lm_dataset, cols={'x_text':LMTensorText, 'y_text':TensorText},hf_toker=hf_tokenizer)\n",
    "lm_dl = MySortedDL(lm_ds, srtkey_fc=False)\n",
    "lm_dl.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ELECTRA data creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>sentA_lenth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] john whispered mary left . mary wonders that bill will come . sophie will theater . john finished the cake and drank the lemon ##ade . herself likes mary ' s mother . each of the boys fought with some of the other boys . john ' s mother likes himself . mary revealed himself to john . mary believes that bill saw himself . [SEP] john heard that they criticized themselves . mary thinks that she is smart . henry found that bill is sad . john considers himself proud of mary . you should sit before there . there is a nurse available . everyone hopes everyone to sleep . everyone hopes that he will sleep . only churchill remembered churchill giving the blood [SEP]</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS] i live at the place where route 150 crosses the hudson river and my dad lives at it too . who is she trying to make up to now ? wind was gotten of a plot to negotiate an honorable end to the war in vietnam . mike talked about politics yesterday to my friends . it was expected by the reporters that the principal would fire some teacher . [SEP] which hat did mike qui ##p that she never wore ? which girl did mike qui ##p never wore this hat ? we donated wire for the convicts to build cages with . i won ' t have some money . do you believe the claim that somebody was looking for something ? i won [SEP]</td>\n      <td>72</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "proc_dset = ELECTRADataTransform(cola['validation'], is_docs=False, text_col='sentence', max_length=128, hf_toker=hf_tokenizer).map()\n",
    "e_dsets = HF_Datasets({'train':proc_dset}, cols={'input_ids':TensorText,'sentA_lenth':noop}, hf_toker=hf_tokenizer)\n",
    "e_dls = e_dsets.dataloaders(srtkey_fc=False)\n",
    "e_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test filtering feature\n",
    "Note that filter won't be applied to split other than train, because validation/test set is for fair comparison, and you can't take out samples at your will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'train': 26, 'validation': 2, 'test': 6}\n"
    }
   ],
   "source": [
    "l = 23\n",
    "num = {}\n",
    "for split in tokenized_cola:\n",
    "  num[split] = reduce(lambda sum, sample: sum+(1 if len(sample['text_idxs'])==l else 0), \n",
    "                      tokenized_cola[split], 0)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|█████████▉| 1061/1063 [00:03<00:00, 340.91it/s]Test passed\n"
    }
   ],
   "source": [
    "ccola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer)\n",
    "ccola_dls = ccola_dsets.dataloaders(filter_fc=lambda text_idxs, label: len(text_idxs)!=l,)\n",
    "\n",
    "for i, split in enumerate(tokenized_cola):\n",
    "  if split == 'train':\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split])-num[split],f\"{split}: filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}, should be filtered: {num[split]}\"\n",
    "  else:\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split]), f\"{split}: accidentally filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cache dataloader\n",
    "If sorting or filtering is applied, dataloader need to create some record inside it, to do it only once, we can cache the records. \n",
    "\n",
    "If `cache_dir` is not specified, it will be the cache_dir of `dsets` passed to `HF_Datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "99%|█████████▉| 1056/1063 [00:03<00:00, 353.91it/s]"
    }
   ],
   "source": [
    "for f in ['/tmp/cached_train.json','/tmp/cached_val.json', '/tmp/cached_test.json']:\n",
    "  if Path(f).exists(): os.remove(f)\n",
    "\n",
    "ccola_dls = ccola_dsets.dataloaders(cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we load the caches, it should be fast and progress bars sholdn't appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccola_dls = ccola_dsets.dataloaders(cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}