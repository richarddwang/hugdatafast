{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There must be a bug in Vscode notebook make me have to import this first, otherwise I get a modulenotfounderror\n",
    "from fastai.callback.core import *\n",
    "\n",
    "from IPython.core.debugger import set_trace as bk\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch\n",
    "import nlp\n",
    "from transformers import ElectraTokenizerFast\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-generator\")\n",
    "from hugdatafast import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple tokenization & infer cache name\n",
    "`cols`(`Dict[str]`): tokenize the every column named key into column named its value  \n",
    "`cols`(`List[str]`): specify the name of columns to be tokenized, replace the original columns' data with tokenized one\n",
    "\n",
    "Here, we tokenized \"sentence\" into a new column named \"text_idxs\", the \"sentence\" column still exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'idx': 0, 'label': 1, 'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\", 'text_idxs': [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]}\n\n/home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4/tokenized_train.arrow\n/home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4/tokenized_validation.arrow\n/home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/637080968c182118f006d3ea39dd9937940e81cfffc8d79836eaae8bba307fc4/tokenized_test.arrow\n"
    }
   ],
   "source": [
    "cola = nlp.load_dataset('glue', 'cola')\n",
    "tokenized_cola = cola.my_map(simple_tokenize_func({'sentence':'text_idxs'}, hf_tokenizer),\n",
    "                             cache_file_names='tokenized_{split}')\n",
    "print(tokenized_cola['train'][0])\n",
    "print()\n",
    "for dset in tokenized_cola.values(): print(dset.cache_files[0]['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fastai `Dataloaders` and `show_batch`\n",
    "\n",
    "`cols`: **specify columns whose values form a output sample in order**, and the semantic type of each column to encode/decode, with one of the following signature (see doc).\n",
    "\n",
    "Here, `['text_idxs, 'label']` is equal to `{'text_idxs': TensorText, 'label': TensorCategory}`\n",
    "\n",
    "The bars are sorting samples according to length, see `MySortedDL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "98%|█████████▊| 1040/1063 [00:01<00:00, 588.25it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever, worked in any office which contained any typewriter which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i finally worked up enough courage to ask which people up at corporate headquarters the sooner i solve this problem, the quicker i'll get free of.</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer, neat_show=True)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32)\n",
    "cola_dls.show_batch(max_n=2) # show at most two rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either specify `neat_show=False` (which is default), to show real data which is tokenized and  with pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "98%|█████████▊| 1042/1063 [00:02<00:00, 397.20it/s]"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_idxs</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everybody who has ever , worked in any office which contained any type ##writer which had ever been used to type any letters which had to be signed by any administrator who ever worked in any department like mine will know what i mean .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the prisoner must have been being inter ##rogated when the supervisor walked into the room and saw what was going on and put a stop to it . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cola_dsets = HF_Datasets(tokenized_cola, cols={'text_idxs': TensorText, 'label': TensorCategory}, hf_toker=hf_tokenizer)\n",
    "cola_dls = cola_dsets.dataloaders(bs=32)\n",
    "cola_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_with_label` is `False` by default, so in test set the sample formed by only first `n_inp` columns specified, which is x.\n",
    "\n",
    "This make you able to apply the same to all splits when test set come with no y or fake y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple columns (> 2) in sample\n",
    "Some points to notice:\n",
    "- title of each column showed is and in order of `cols` specified in `HF_Datasets`\n",
    "- auto pad sequence to the max length in the batch, for all columns\n",
    "- If a fastai semantic tensor type is not specified, it look dtype and shape of the tensor and decide how to decode it autmatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'idx': 0, 'label': 0, 'span1_index': 0, 'span1_text': 'Mark', 'span2_index': 13, 'span2_text': 'He', 'text': 'Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical.'}\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>span1_index</th>\n      <th>span1_text</th>\n      <th>span2_index</th>\n      <th>span2_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mark told pete many lies about himself , which pete included in his book . he should have been more skeptical . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>0</td>\n      <td>mark [PAD] [PAD]</td>\n      <td>13</td>\n      <td>he</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the mothers of arthur and celeste have come to the town to fetch them . they are very happy to have them back , but they sc ##old them just the same because they ran away . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</td>\n      <td>1</td>\n      <td>mothers [PAD] [PAD]</td>\n      <td>25</td>\n      <td>them</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mark was close to mr . singer ' s heels . he heard him calling for the captain , promising him , in the jar ##gon everyone talked that night , that not one thing should be damaged on the ship except only the ammunition , but the captain and all his crew had best stay in the cabin until the work was over</td>\n      <td>4</td>\n      <td>mr . singer</td>\n      <td>8</td>\n      <td>he</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "wsc = nlp.load_dataset('super_glue', 'wsc.fixed')\n",
    "print(wsc['train'][0])\n",
    "tokenized_wsc = wsc.my_map(simple_tokenize_func(['text', 'span1_text', 'span2_text'], hf_tokenizer))\n",
    "wsc_dsets = HF_Datasets(tokenized_wsc, cols={'text': TensorText, 'span1_index': noop, 'span1_text':TensorText, 'span2_index': noop, 'span2_text': TensorText, 'label': lambda t: t.bool()}, # convert label (int) to (bool), just to test its abililty to show tensor(bool)\n",
    "hf_toker=hf_tokenizer)\n",
    "dls = wsc_dsets.dataloaders(bs=3, srtkey_fc=False, shuffle_train=False) # don't sort samples, don't shuffle trainset\n",
    "#bk()\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aggregate Dataset\n",
    "a sample in transformed dataset is aggregated/accumulated from multiple original samples.\n",
    "\n",
    "- Except for `LMTransform`, you can implement your own logic create a class inherits `AggregateTransform` and implements `accumulate` and `create_example` method\n",
    "\n",
    "- Note that you should pass **tokenized** dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make  dataset for (traditional) language model\n",
    "You can always pass dict of `nlp.Dataset` or a `nlp.Dataset` at your will for any transform class, we've test passing a dict, now we test a `nlp.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original dataset:\nnum of samples: 1043\nsecond to last sentence: John arranged for himself to get the prize.\n          last sentence: John talked to Bill about himself.\nLM dataset:\nnum of sampels: 481\nlast text (x): . john talked to bill about himself\nlast text (y): john talked to bill about himself.\n"
    }
   ],
   "source": [
    "cola_val = tokenized_cola['validation']\n",
    "lm_cola_val = LMTransform(cola_val, max_len=20, text_col='text_idxs').map()\n",
    "\n",
    "print('Original dataset:')\n",
    "print('num of samples:', len(cola['validation']))\n",
    "print('second to last sentence:', cola['validation'][-2]['sentence'])\n",
    "print('          last sentence:', cola['validation'][-1]['sentence'])\n",
    "print('LM dataset:')\n",
    "print('num of sampels:', len(lm_cola_val))\n",
    "assert len(lm_cola_val) == 481\n",
    "print('last text (x):', hf_tokenizer.decode(lm_cola_val[-1]['x_text']))\n",
    "print('last text (y):', hf_tokenizer.decode(lm_cola_val[-1]['y_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_text</th>\n      <th>y_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey . the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as you eat the most , you want the</td>\n      <td>sailors rode the breeze clear of the rocks . the weights made the rope stretch over the pull ##ey . the mechanical doll wr ##ig ##gled itself loose . if you had eaten more , you would want less . as you eat the most , you want the least</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>. the more you would want , the less you would eat . i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead , she gets depressed . the ang ##rier mary got , the more she looked at pictures</td>\n      <td>the more you would want , the less you would eat . i demand that the more john eat , the more he pays . mary listen ##s to the grateful dead , she gets depressed . the ang ##rier mary got , the more she looked at pictures .</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "lm_cola = LMTransform(tokenized_cola, max_len=50, text_col='text_idxs').map()\n",
    "# test single dataset\n",
    "lm_ds = HF_Dataset(lm_cola['validation'], cols={'x_text':LMTensorText, 'y_text':TensorText},hf_toker=hf_tokenizer)\n",
    "lm_dl = MySortedDL(lm_ds, srtkey_fc=False)\n",
    "lm_dl.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ELECTRA data creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>sentA_lenth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] herman whipped the sugar and the cream . linda taped the picture to the wall . linda taped the picture onto the wall . the child and her mother clung together . this flyer and that flyer differ . this flyer and that flyer differ apart . the jewel ##ler sc ##ri ##bbled the contract with his name . the gardener grew that ac ##orn into an oak tree . [SEP] i shaped a loaf . the children amused . susan whispered the news . susan whispered at rachel . ellen said that mel ##ons were selling well . ellen said about the present conditions . ellen warned helen against skating on thin ice . cynthia ni ##bbled on the carrot . cynthia chewed . paul [SEP]</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS] the mayor regarded as being absurd the proposal to build a sidewalk from dartmouth to smith . i want that bill left to remain a secret . i know a man who tom drives as drives . drowning cats , which is against the law , are hard to rescue . himself is understood by rutherford . [SEP] muriel said nothing else than that she had been insulted . i feel that arch will show up . the proof this set is rec ##urs ##ive is difficult . the mad ##ri ##gal ##s which henry plays the lu ##te and sings sound lou ##sy . tom picked these grapes , and i washed some turn ##ip ##s , and su ##zie will prepare these grapes . [SEP]</td>\n      <td>59</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "proc_dset = ELECTRADataTransform(cola['validation'], is_docs=False, text_col='sentence', max_length=128, hf_toker=hf_tokenizer).map()\n",
    "e_dsets = HF_Datasets({'train':proc_dset}, cols={'input_ids':TensorText,'sentA_lenth':noop}, hf_toker=hf_tokenizer)\n",
    "e_dls = e_dsets.dataloaders(srtkey_fc=False)\n",
    "e_dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test filtering feature\n",
    "Note that filter won't be applied to split other than train, because validation/test set is for fair comparison, and you can't take out samples at your will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'train': 26, 'validation': 2, 'test': 6}\n"
    }
   ],
   "source": [
    "l = 23\n",
    "num = {}\n",
    "for split in tokenized_cola:\n",
    "  num[split] = reduce(lambda sum, sample: sum+(1 if len(sample['text_idxs'])==l else 0), \n",
    "                      tokenized_cola[split], 0)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "96%|█████████▋| 1025/1063 [00:02<00:00, 380.90it/s]Test passed\n"
    }
   ],
   "source": [
    "ccola_dsets = HF_Datasets(tokenized_cola, cols=['text_idxs', 'label'], hf_toker=hf_tokenizer)\n",
    "ccola_dls = ccola_dsets.dataloaders(filter_fc=lambda text_idxs, label: len(text_idxs)!=l,)\n",
    "\n",
    "for i, split in enumerate(tokenized_cola):\n",
    "  if split == 'train':\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split])-num[split],f\"{split}: filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}, should be filtered: {num[split]}\"\n",
    "  else:\n",
    "    assert ccola_dls[i].n == len(tokenized_cola[split]), f\"{split}: accidentally filtered: {ccola_dls[i].n}, unfiltered: {len(tokenized_cola[split])}\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cache dataloader\n",
    "If sorting or filtering is applied, dataloader need to create some record inside it, to do it only once, we can cache the records. \n",
    "\n",
    "If `cache_dir` is not specified, it will be the cache_dir of `dsets` passed to `HF_Datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "99%|█████████▊| 1049/1063 [00:02<00:00, 388.21it/s]"
    }
   ],
   "source": [
    "for f in ['/tmp/cached_train.json','/tmp/cached_val.json', '/tmp/cached_test.json']:\n",
    "  if Path(f).exists(): os.remove(f)\n",
    "\n",
    "ccola_dls = ccola_dsets.dataloaders(cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we load the caches, it should be fast and progress bars sholdn't appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccola_dls = ccola_dsets.dataloaders(cache_dir='/tmp', cache_name='cached_{split}.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}